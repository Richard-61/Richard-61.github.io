<!DOCTYPE html>
<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Yi Liu (刘熠)</title>
	<meta content="Yi Liu (刘熠), Richard-61.github.io" name="keywords" />
	<style media="screen" type="text/css">
  html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: Arial, Helvetica, sans-serif;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  margin: .2em auto .2em auto;
  color: #137fd1;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 2em auto 2em auto;
  width: 950px;
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 15px;
  background: #F4F6F6;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 17pt;
  font-weight: 700;
}

h3 {
  margin: 0.1em auto 1em auto;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

h4 {
  margin: 0.1em auto 0.3em auto;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-weight:bold;
}

ul { 
  list-style: disc;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Arial, Helvetica, sans-serif;
  font-size: 14px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.2em;
  background: #F4F6F6;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 2px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}

div.paper:hover {
    background: #FFFDEE;
}

div.paper2 {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}

div.paper2:hover {
    background: #FFFDEE;
}

div.bio {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 135%;
}

div.res {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.award {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.paper div {
  padding-left: 270px;
}

img.paper {
  float: left;
  width: 250px;
  height: 100px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 14px;
  margin: 1em 0;
  padding: 0;
}

.bot {
  font-size: 14%;
}

.ptypej {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
    background-color: #5cb85c;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
    margin-right: 6px;
}

.ptypec {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
    background-color: #5018a9;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
    margin-right: 6px;
}

.ptypep {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
    background-color: #6B6B6B;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
    margin-right: 6px;
}

/* navigation */
#nav {
  font-family: Georgia, Helvetica, sans-serif;
  position: fixed;
  top: 50px;
  margin-left: 860px;
  width: 118px;
  font-size: 15px;
}

#nav li2 {
    margin-bottom: 1px;
}

ol {
  list-style: none;
}

#nav a {
    display: block;
    padding: 6px 9px 7px;
    color: #fff;
    background-color: #000000;
    text-decoration: none;
}

#nav a:hover {
    color: #ffde00;
}

/* 专门为论文列表设计的样式 */
.publication-section {
  padding: 1.2em;
  background: #fff;
  border: 2px solid #ddd;
  border-radius: 10px;
  margin-bottom: 1em;
}

.publication-section:hover {
  background: #FFFDEE;
}

.paper-category {
  font-size: 1.1em;
  font-weight: bold;
  color: #cd1313;
  margin-bottom: 1em;
  display: block;
}

.paper-list {
  list-style: none;
  padding: 0;
  margin: 0;
}

/* 为每个分类创建独立的计数器 */
.multimodal-list {
  counter-reset: multimodal-counter;
}

.traditional-list {
  counter-reset: traditional-counter;
}

.paper-entry {
  display: flex;
  margin-bottom: 0.6em;
  line-height: 1.3;
}

/* 自动编号功能 */
.multimodal-list .paper-entry {
  counter-increment: multimodal-counter;
}

.traditional-list .paper-entry {
  counter-increment: traditional-counter;
}

.paper-number {
  font-weight: bold;
  color: #000000;
  margin-right: 1em;
  flex-shrink: 0;
  width: 30px;
}

/* 使用伪元素自动生成编号 */
.multimodal-list .paper-number::before {
  content: "[" counter(multimodal-counter) "]";
}

.traditional-list .paper-number::before {
  content: "[" counter(traditional-counter) "]";
}

.paper-content {
  flex: 1;
}

.paper-content a {
  font-weight: bold;
  color: #108be9;
}

.paper-content a:hover {
  color: #f09228;
}

.paper-venue {
  font-style: italic;
}

.paper-note {
  color: #f09228;
  font-weight: bold;
}

/* 修复原有paper div的样式冲突 */
.publication-section > div {
  padding-left: 0 !important;
}
</style>

<script type="text/javascript" src="./files/hidebib.js"></script>
<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
</head>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');
</script>

<body>
  <ol id="nav">
    <li><a href="#home" title="Home">Home</a></li>
    <li><a href="#pub" title="Publications">Publications</a></li>
    <li><a href="#exp" title="Experience">Experience</a></li>
  </ol>

<a name="home"></a>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 170px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Yi Liu (刘熠)" style="float: left; padding-left: 1em; padding-right: 2em; height: 170px;" src="yiliu.jpg" />
<div style="padding-left: 1em; vertical-align: top; height: 170px;"><span style="line-height: 150%; font-size: 23pt;">Yi Liu (刘熠)</span><br />
<a href="https://scholar.google.com/citations?user=gGPehK4AAAAJ&hl=en"><strong>[Google Scholar]</strong></a> &nbsp;&nbsp;&nbsp;
<a href="https://github.com/Richard-61"><strong>[GitHub]</strong></a>&nbsp;&nbsp;&nbsp;
<br></br>
<h4><strong>Senior Engineer at  <i> Honor Device Co., Ltd</i>  .</strong></h4>
<h4><span><strong>Personal Email </strong>:  <u> yiliu61richard@gmail.com</u> </span> <br /></h4>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2>About Me</h2>
<div class="bio">
<p>
Now I work at Honor Device Co., Ltd. as the Project Lead (PL) of the On-device VLM Group, focusing on Vision-Language Models (VLM) and video understanding.
I received my Ph.D. degree at <a href='http://mmlab.siat.ac.cn/'>MMLab@SIAT</a>, 
<a href='http://english.ucas.ac.cn/'>University of Chinese Academy of Sciences</a>, supervised by 
<a href='https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=zh-CN'>Prof. Yu Qiao</a>
and <a href='https://scholar.google.com/citations?user=hD948dkAAAAJ&hl=zh-CN'>Prof. Yali Wang</a> in 2024.
And I was a research intern at <a href="https://www.shlab.org.cn/">Shanghai AI Laboratory </a> from 2022 to 2023. 
I received a B.Eng. degree in <a href='https://english.hust.edu.cn/'>Huazhong University of Science and Technology (HUST)</a>, Wuhan, China, in 2019.<br>

<b>My research interests broadly lie in the areas of  Computer Vision and Deep Learning, including:  </b>
<br />
<strong >1> MultiModal Learning</strong>: Video-Language Modeling, Multimodal Large Language Model
<br />
<strong >2> Video-related Applications</strong>: Temporal Action Detection, Temporal Sentence Grounding
<br />
<strong >3> Long-form Video Understanding</strong>: Online Action Detection, Video QA, Video Caption
</p>
</div>
</div>

<a name="pub"></a>
<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Publications</h2>

<div class="publication-section">
  <span class="paper-category">Multimodal-LLM:</span>
  
  <div class="paper-list multimodal-list">
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="http://arxiv.org/abs/2508.01540">MagicVL-2B: Empowering Vision-Language Models on Mobile Devices with Lightweight Visual Encoders via Curriculum Learning</a>,
        <span class="paper-venue">arXiv 2025</span> <span class="paper-note">(AAAI 2026 under review, 第1作者)</span>
      </div>
    </div>
    
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <b>MagicGen: A Universal Multimodal Data Synthesis Agent for Domain-Specific Vision-Language Model Tuning</b>,
        <span class="paper-venue">arXiv 2025</span> <span class="paper-note">(In process, 第1通讯)</span>
      </div>
    </div>
    
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="http://arxiv.org/abs/2508.01546">E-VRAG: Enhancing Long Video Understanding with Resource-Efficient Retrieval Augmented Generation</a>,
        <span class="paper-venue">arXiv 2025</span> <span class="paper-note">(AAAI 2026 under review, 第1通讯)</span>
      </div>
    </div>
    
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="https://arxiv.org/abs/2506.01725">VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking</a>,
        <span class="paper-venue">arXiv 2025</span> <span class="paper-note">(NeurIPS 2025 under review, 第2通讯)</span>
      </div>
    </div>
    
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/10777595">MLLM-TA: Leveraging Multimodal Large Language Models for Precise Temporal Video Grounding</a>,
        <span class="paper-venue">IEEE Signal Processing Letters, 2024</span> <span class="paper-note">(SPL, IF=3.9, 第1作者)</span>
      </div>
    </div>
    
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="https://arxiv.org/abs/2311.17005">MVBench: A Comprehensive Multi-modal Video Understanding Benchmark</a>,
        <span class="paper-venue">CVPR 2024</span> <span class="paper-note">(CVPR, CCF-A, 第6作者)</span>
      </div>
    </div>
    
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="https://arxiv.org/abs/2312.04817">LvBench: A Benchmark for Long-form Video Understanding with Versatile Multi-modal Question Answering</a>,
        <span class="paper-venue">arXiv 2023</span> <span class="paper-note">(IJCV under review, 第2作者)</span>
      </div>
    </div>
  </div>

  <span class="paper-category" style="margin-top: 2em;">Traditional Video Understanding:</span>
  <div class="paper-list traditional-list">
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="https://link.springer.com/article/10.1007/s11554-024-01454-4">F2S-Net: Learning Frame-To-Segment Prediction for Online Action Detection</a>,
        <span class="paper-venue">Journal of Real-Time Image Processing, 2024</span> <span class="paper-note">(JRTIP, IF=3.0, 第1作者)</span>
      </div>
    </div>

    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/10374139">Dual masked modeling for weakly-supervised temporal boundary discovery</a>,
        <span class="paper-venue">IEEE Transactions on Multimedia, 2023</span> <span class="paper-note">(TMM, IF=9.7, 第2作者)</span>
      </div>
    </div>
    
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="https://arxiv.org/abs/2212.03191">InternVideo: General Video Foundation Models via Generative and Discriminative Learning</a>,
        <span class="paper-venue">arXiv 2022</span> <span class="paper-note">(SCIS under review, 第9作者)</span>
      </div>
    </div>
    
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611824">Learning Discriminative Feature Representation for Open Set Action Recognition</a>,
        <span class="paper-venue">ACM International Conference on Multimedia, 2023</span> <span class="paper-note">(ACM MM, CCF-A, 第2作者)</span>
      </div>
    </div>
    
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="https://arxiv.org/abs/2105.11107">FineAction: A Fine-Grained Video Dataset for Temporal Action Localization</a>,
        <span class="paper-venue">IEEE Transactions on Image Processing, 2022</span> <span class="paper-note">(TIP, IF=13.7, 第1作者)</span>
      </div>
    </div>
    
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="https://arxiv.org/abs/2210.11158">VideoPipe 2022 Challenge: Real-World Video Understanding for Urban Pipe Inspection</a>,
        <span class="paper-venue">International Conference on Pattern Recognition, 2022</span> <span class="paper-note">(ICPR, CCF-C, 第1作者)</span>
      </div>
    </div>
    
    <div class="paper-entry">
      <span class="paper-number"></span>
      <div class="paper-content">
        <a href="https://jcjs.siat.ac.cn/cn/article/id/202106006">短视频场景在线起始检测任务及方法研究</a>,
        <span class="paper-venue">集成技术, 2021</span> <span class="paper-note">(第2作者)</span>
      </div>
    </div>
  </div>
</div>

</div>
</div>

<a name="exp"></a>
<div style="clear: both;">
<div class="section"><h2>Experience</h2>
<div class="paper">

<h3>Workshops</h3>
<li> <b >Student organizer</b>  of ECCV 2022 DeeperAction Challenge, <b> <a href='https://codalab.lisn.upsaclay.fr/competitions/4386'>Track 1: Temporal Action Localization</a>  </b>  </li>
<li> <b >Student organizer</b>  of ICPR 2022 VideoPipe Challenge,    <b> <a href='https://codalab.lisn.upsaclay.fr/competitions/2284'>Track 2: Temporal Defect Localization</a>  </b> </li>
<li> <b >Student organizer</b>  of ICCV 2021 DeeperAction Challenge, <b> <a href='https://competitions.codalab.org/competitions/32363'>Track 1: Temporal Action Localization</a> </b>  </li> 
<li> <b >1st Prize</b> in ECCV 2022 Ego4D Episodic Memory Challenge, <b> <a href='https://ego4d-data.org/workshops/eccv22/'> Moments Queries Track </a>  </b>  </li>
<li> <b >1st Prize</b> in ECCV 2022 Ego4D Episodic Memory Challenge, <b> <a href='https://ego4d-data.org/workshops/eccv22/'> Looking At Me Track </a>  </b> </li>

<h3>Journal Reviewer</h3>
<li>  Neural Networks, Journal of Visual Communication and Image Representation </li>
<li>  Pattern Recognition, International Journal of Computer Vision  </li>
<li>  IEEE Transactions on Pattern Analysis and Machine Intelligence  </li>

</div>
</div>
</div>	

</body>
</html>